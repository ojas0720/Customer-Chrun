{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Customer Churn Prediction - Model Training\n",
                "\n",
                "This notebook demonstrates the complete model training workflow for the Customer Churn Prediction System.\n",
                "\n",
                "## Overview\n",
                "\n",
                "We'll walk through:\n",
                "1. Loading and validating training data\n",
                "2. Data preprocessing and feature engineering\n",
                "3. Training an XGBoost model\n",
                "4. Evaluating model performance\n",
                "5. Saving the model with versioning"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "\n",
                "First, let's import the required libraries and services."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "\n",
                "# Import our custom services\n",
                "from services.preprocessing import DataValidator, DataTransformer, FeatureEngineer\n",
                "from services.model_training import ModelTrainer\n",
                "from services.model_repository import ModelRepository\n",
                "\n",
                "# Set display options\n",
                "pd.set_option('display.max_columns', None)\n",
                "sns.set_style('whitegrid')\n",
                "\n",
                "print(\"✓ All imports successful\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Load and Explore Data\n",
                "\n",
                "Let's load the training and test datasets and examine their structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "train_df = pd.read_csv('../data/raw/training_data.csv')\n",
                "test_df = pd.read_csv('../data/raw/test_data.csv')\n",
                "\n",
                "print(f\"Training data shape: {train_df.shape}\")\n",
                "print(f\"Test data shape: {test_df.shape}\")\n",
                "print(f\"\\nTraining data churn rate: {train_df['churn'].mean():.2%}\")\n",
                "print(f\"Test data churn rate: {test_df['churn'].mean():.2%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display first few rows\n",
                "train_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check data types and missing values\n",
                "print(\"Data Info:\")\n",
                "print(train_df.info())\n",
                "print(f\"\\nMissing values:\\n{train_df.isnull().sum()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize churn distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "# Churn distribution\n",
                "train_df['churn'].value_counts().plot(kind='bar', ax=axes[0], color=['green', 'red'])\n",
                "axes[0].set_title('Churn Distribution')\n",
                "axes[0].set_xlabel('Churn (0=No, 1=Yes)')\n",
                "axes[0].set_ylabel('Count')\n",
                "axes[0].set_xticklabels(['No Churn', 'Churn'], rotation=0)\n",
                "\n",
                "# Monthly charges distribution by churn\n",
                "train_df.boxplot(column='monthly_charges', by='churn', ax=axes[1])\n",
                "axes[1].set_title('Monthly Charges by Churn Status')\n",
                "axes[1].set_xlabel('Churn (0=No, 1=Yes)')\n",
                "axes[1].set_ylabel('Monthly Charges')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Data Validation\n",
                "\n",
                "Validate that the data meets our requirements."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize validator\n",
                "validator = DataValidator()\n",
                "\n",
                "# Validate training data\n",
                "try:\n",
                "    validator.validate(train_df)\n",
                "    print(\"✓ Training data validation passed\")\n",
                "except Exception as e:\n",
                "    print(f\"✗ Training data validation failed: {e}\")\n",
                "\n",
                "# Validate test data\n",
                "try:\n",
                "    validator.validate(test_df)\n",
                "    print(\"✓ Test data validation passed\")\n",
                "except Exception as e:\n",
                "    print(f\"✗ Test data validation failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Feature Engineering\n",
                "\n",
                "Create derived features to improve model performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize feature engineer\n",
                "engineer = FeatureEngineer()\n",
                "\n",
                "# Engineer features for training data\n",
                "train_engineered = engineer.engineer_features(train_df.copy())\n",
                "test_engineered = engineer.engineer_features(test_df.copy())\n",
                "\n",
                "print(f\"Original features: {train_df.shape[1]}\")\n",
                "print(f\"After feature engineering: {train_engineered.shape[1]}\")\n",
                "print(f\"\\nNew features added: {train_engineered.shape[1] - train_df.shape[1]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display engineered features\n",
                "new_columns = [col for col in train_engineered.columns if col not in train_df.columns]\n",
                "print(f\"New features: {new_columns}\")\n",
                "train_engineered[new_columns].head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Data Preprocessing\n",
                "\n",
                "Transform the data for model training (encoding, scaling, etc.)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate features and target\n",
                "X_train = train_engineered.drop('churn', axis=1)\n",
                "y_train = train_engineered['churn']\n",
                "X_test = test_engineered.drop('churn', axis=1)\n",
                "y_test = test_engineered['churn']\n",
                "\n",
                "print(f\"X_train shape: {X_train.shape}\")\n",
                "print(f\"y_train shape: {y_train.shape}\")\n",
                "print(f\"X_test shape: {X_test.shape}\")\n",
                "print(f\"y_test shape: {y_test.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize and fit transformer\n",
                "transformer = DataTransformer()\n",
                "X_train_transformed = transformer.fit_transform(X_train, y_train)\n",
                "X_test_transformed = transformer.transform(X_test)\n",
                "\n",
                "print(f\"Transformed training data shape: {X_train_transformed.shape}\")\n",
                "print(f\"Transformed test data shape: {X_test_transformed.shape}\")\n",
                "print(f\"\\n✓ Data preprocessing complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Train XGBoost Model\n",
                "\n",
                "Train the churn prediction model with optimized hyperparameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize trainer\n",
                "trainer = ModelTrainer()\n",
                "\n",
                "# Train model\n",
                "print(\"Training XGBoost model...\")\n",
                "model = trainer.train(X_train_transformed, y_train)\n",
                "print(\"✓ Model training complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Evaluate Model Performance\n",
                "\n",
                "Assess the model's performance on the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on test set\n",
                "metrics = trainer.evaluate(model, X_test_transformed, y_test)\n",
                "\n",
                "print(\"Model Performance Metrics:\")\n",
                "print(f\"  Precision: {metrics['precision']:.4f}\")\n",
                "print(f\"  Recall: {metrics['recall']:.4f}\")\n",
                "print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
                "print(f\"  Accuracy: {metrics.get('accuracy', 'N/A')}\")\n",
                "\n",
                "# Check if recall meets threshold\n",
                "if metrics['recall'] >= 0.85:\n",
                "    print(f\"\\n✓ Model meets recall threshold (>= 85%)\")\n",
                "else:\n",
                "    print(f\"\\n⚠ Model recall ({metrics['recall']:.2%}) is below 85% threshold\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate predictions for confusion matrix\n",
                "y_pred = model.predict(X_test_transformed)\n",
                "y_pred_proba = model.predict_proba(X_test_transformed)[:, 1]\n",
                "\n",
                "# Confusion matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "\n",
                "# Visualize confusion matrix\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=['No Churn', 'Churn'],\n",
                "            yticklabels=['No Churn', 'Churn'])\n",
                "plt.title('Confusion Matrix')\n",
                "plt.ylabel('Actual')\n",
                "plt.xlabel('Predicted')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nTrue Negatives: {cm[0, 0]}\")\n",
                "print(f\"False Positives: {cm[0, 1]}\")\n",
                "print(f\"False Negatives: {cm[1, 0]}\")\n",
                "print(f\"True Positives: {cm[1, 1]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification report\n",
                "print(\"\\nDetailed Classification Report:\")\n",
                "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Probability distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Histogram of predicted probabilities\n",
                "axes[0].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.5, label='No Churn', color='green')\n",
                "axes[0].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.5, label='Churn', color='red')\n",
                "axes[0].axvline(x=0.5, color='black', linestyle='--', label='Threshold')\n",
                "axes[0].set_xlabel('Predicted Probability')\n",
                "axes[0].set_ylabel('Frequency')\n",
                "axes[0].set_title('Distribution of Predicted Probabilities')\n",
                "axes[0].legend()\n",
                "\n",
                "# Feature importance\n",
                "feature_importance = model.feature_importances_\n",
                "feature_names = transformer.get_feature_names()\n",
                "importance_df = pd.DataFrame({\n",
                "    'feature': feature_names,\n",
                "    'importance': feature_importance\n",
                "}).sort_values('importance', ascending=False).head(10)\n",
                "\n",
                "axes[1].barh(importance_df['feature'], importance_df['importance'])\n",
                "axes[1].set_xlabel('Importance')\n",
                "axes[1].set_title('Top 10 Feature Importances')\n",
                "axes[1].invert_yaxis()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Save Model with Versioning\n",
                "\n",
                "Save the trained model and transformer to the model repository."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize repository\n",
                "repo = ModelRepository()\n",
                "\n",
                "# Save model with metadata\n",
                "version = repo.save(model, transformer, metrics)\n",
                "\n",
                "print(f\"✓ Model saved successfully\")\n",
                "print(f\"  Version: {version}\")\n",
                "print(f\"  Recall: {metrics['recall']:.4f}\")\n",
                "print(f\"  Precision: {metrics['precision']:.4f}\")\n",
                "print(f\"  F1-Score: {metrics['f1_score']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List all available versions\n",
                "versions = repo.list_versions()\n",
                "\n",
                "print(f\"\\nTotal model versions: {len(versions)}\")\n",
                "print(\"\\nAvailable versions:\")\n",
                "for v in versions[-5:]:  # Show last 5 versions\n",
                "    print(f\"  {v.version}\")\n",
                "    print(f\"    Recall: {v.metadata.get('recall', 'N/A')}\")\n",
                "    print(f\"    Timestamp: {v.metadata.get('timestamp', 'N/A')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "In this notebook, we:\n",
                "\n",
                "1. ✓ Loaded and explored customer churn data\n",
                "2. ✓ Validated data quality and schema\n",
                "3. ✓ Engineered derived features\n",
                "4. ✓ Preprocessed data (encoding, scaling)\n",
                "5. ✓ Trained an XGBoost classifier\n",
                "6. ✓ Evaluated model performance\n",
                "7. ✓ Saved the model with versioning\n",
                "\n",
                "The trained model is now ready for making predictions. See the next notebook (`02_prediction_and_explanation.ipynb`) to learn how to use the model for inference and generate SHAP explanations."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "- **Hyperparameter Tuning**: Experiment with different XGBoost parameters to improve performance\n",
                "- **Feature Selection**: Analyze feature importance and remove low-impact features\n",
                "- **Cross-Validation**: Use k-fold cross-validation for more robust evaluation\n",
                "- **Threshold Optimization**: Adjust the classification threshold based on business requirements\n",
                "- **Make Predictions**: Use the trained model in the prediction notebook"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}