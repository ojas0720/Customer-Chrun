{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Customer Churn Prediction - Model Evaluation and Monitoring\n",
                "\n",
                "This notebook demonstrates how to evaluate model performance, monitor for degradation, and manage model versions.\n",
                "\n",
                "## Overview\n",
                "\n",
                "We'll cover:\n",
                "1. Loading and evaluating models\n",
                "2. Computing performance metrics\n",
                "3. Analyzing confusion matrices\n",
                "4. Checking model calibration\n",
                "5. Storing predictions for validation\n",
                "6. Detecting performance degradation\n",
                "7. Managing model versions and rollback"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
                "from sklearn.calibration import calibration_curve\n",
                "\n",
                "# Import our services\n",
                "from services.prediction import ChurnPredictor\n",
                "from services.monitoring import ModelEvaluator, AlertService, PredictionStore\n",
                "from services.model_repository import ModelRepository\n",
                "\n",
                "# Set display options\n",
                "pd.set_option('display.max_columns', None)\n",
                "sns.set_style('whitegrid')\n",
                "\n",
                "print(\"✓ All imports successful\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Load Model and Test Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize predictor\n",
                "predictor = ChurnPredictor()\n",
                "\n",
                "print(f\"✓ Loaded model version: {predictor.model_version}\")\n",
                "\n",
                "# Load test data\n",
                "test_data = pd.read_csv('../data/raw/test_data.csv')\n",
                "X_test = test_data.drop('churn', axis=1)\n",
                "y_test = test_data['churn']\n",
                "\n",
                "print(f\"Test data shape: {test_data.shape}\")\n",
                "print(f\"Churn rate: {y_test.mean():.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Evaluate Model Performance\n",
                "\n",
                "Let's compute comprehensive performance metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize evaluator\n",
                "evaluator = ModelEvaluator()\n",
                "\n",
                "# Evaluate model\n",
                "metrics = evaluator.evaluate(predictor.model, predictor.transformer, X_test, y_test)\n",
                "\n",
                "print(\"Model Performance Metrics:\")\n",
                "print(f\"  Precision: {metrics['precision']:.4f}\")\n",
                "print(f\"  Recall: {metrics['recall']:.4f}\")\n",
                "print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
                "print(f\"  Accuracy: {metrics.get('accuracy', 'N/A')}\")\n",
                "\n",
                "# Check if meets threshold\n",
                "MIN_RECALL = 0.85\n",
                "if metrics['recall'] >= MIN_RECALL:\n",
                "    print(f\"\\n✓ Model meets recall threshold (>= {MIN_RECALL:.0%})\")\n",
                "else:\n",
                "    print(f\"\\n⚠ Model recall is below {MIN_RECALL:.0%} threshold\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Confusion Matrix Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate predictions\n",
                "y_pred = predictor.model.predict(predictor.transformer.transform(X_test))\n",
                "y_pred_proba = predictor.model.predict_proba(predictor.transformer.transform(X_test))[:, 1]\n",
                "\n",
                "# Compute confusion matrix\n",
                "cm = evaluator.compute_confusion_matrix(y_test, y_pred)\n",
                "\n",
                "print(\"Confusion Matrix:\")\n",
                "print(cm)\n",
                "print(f\"\\nBreakdown:\")\n",
                "print(f\"  True Negatives (correctly predicted no churn): {cm[0, 0]}\")\n",
                "print(f\"  False Positives (predicted churn, but didn't): {cm[0, 1]}\")\n",
                "print(f\"  False Negatives (predicted no churn, but did): {cm[1, 0]}\")\n",
                "print(f\"  True Positives (correctly predicted churn): {cm[1, 1]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize confusion matrix\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=['No Churn', 'Churn'],\n",
                "            yticklabels=['No Churn', 'Churn'],\n",
                "            cbar_kws={'label': 'Count'})\n",
                "plt.title('Confusion Matrix')\n",
                "plt.ylabel('Actual')\n",
                "plt.xlabel('Predicted')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Calculate rates\n",
                "tn, fp, fn, tp = cm.ravel()\n",
                "total = tn + fp + fn + tp\n",
                "\n",
                "print(f\"\\nRates:\")\n",
                "print(f\"  True Negative Rate (Specificity): {tn/(tn+fp):.2%}\")\n",
                "print(f\"  False Positive Rate: {fp/(tn+fp):.2%}\")\n",
                "print(f\"  True Positive Rate (Recall/Sensitivity): {tp/(tp+fn):.2%}\")\n",
                "print(f\"  False Negative Rate: {fn/(tp+fn):.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: ROC Curve and AUC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute ROC curve\n",
                "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
                "roc_auc = auc(fpr, tpr)\n",
                "\n",
                "# Plot ROC curve\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
                "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
                "plt.xlim([0.0, 1.0])\n",
                "plt.ylim([0.0, 1.05])\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate (Recall)')\n",
                "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
                "plt.legend(loc=\"lower right\")\n",
                "plt.grid(alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"AUC Score: {roc_auc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Model Calibration\n",
                "\n",
                "Check if predicted probabilities match actual outcomes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute calibration curve\n",
                "prob_true, prob_pred = evaluator.compute_calibration_curve(y_test, y_pred_proba)\n",
                "\n",
                "# Plot calibration curve\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.plot(prob_pred, prob_true, marker='o', linewidth=2, label='Model')\n",
                "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')\n",
                "plt.xlabel('Predicted Probability')\n",
                "plt.ylabel('True Probability')\n",
                "plt.title('Calibration Curve')\n",
                "plt.legend()\n",
                "plt.grid(alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Calibration Analysis:\")\n",
                "print(\"If the curve is close to the diagonal, the model is well-calibrated.\")\n",
                "print(\"Above diagonal = model underestimates probability\")\n",
                "print(\"Below diagonal = model overestimates probability\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Probability Distribution Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze probability distributions\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# 1. Histogram by actual class\n",
                "axes[0, 0].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.6, label='No Churn', color='green')\n",
                "axes[0, 0].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.6, label='Churn', color='red')\n",
                "axes[0, 0].axvline(x=0.5, color='black', linestyle='--', label='Threshold')\n",
                "axes[0, 0].set_xlabel('Predicted Probability')\n",
                "axes[0, 0].set_ylabel('Frequency')\n",
                "axes[0, 0].set_title('Probability Distribution by Actual Class')\n",
                "axes[0, 0].legend()\n",
                "\n",
                "# 2. Box plot by actual class\n",
                "prob_df = pd.DataFrame({\n",
                "    'probability': y_pred_proba,\n",
                "    'actual': ['Churn' if y == 1 else 'No Churn' for y in y_test]\n",
                "})\n",
                "prob_df.boxplot(column='probability', by='actual', ax=axes[0, 1])\n",
                "axes[0, 1].set_title('Probability Distribution by Actual Class')\n",
                "axes[0, 1].set_xlabel('Actual Class')\n",
                "axes[0, 1].set_ylabel('Predicted Probability')\n",
                "\n",
                "# 3. Cumulative distribution\n",
                "sorted_probs = np.sort(y_pred_proba)\n",
                "cumulative = np.arange(1, len(sorted_probs) + 1) / len(sorted_probs)\n",
                "axes[1, 0].plot(sorted_probs, cumulative, linewidth=2)\n",
                "axes[1, 0].axvline(x=0.5, color='red', linestyle='--', label='Threshold')\n",
                "axes[1, 0].set_xlabel('Predicted Probability')\n",
                "axes[1, 0].set_ylabel('Cumulative Proportion')\n",
                "axes[1, 0].set_title('Cumulative Distribution of Probabilities')\n",
                "axes[1, 0].legend()\n",
                "axes[1, 0].grid(alpha=0.3)\n",
                "\n",
                "# 4. Probability bins\n",
                "bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
                "prob_df['bin'] = pd.cut(prob_df['probability'], bins=bins)\n",
                "bin_counts = prob_df.groupby('bin')['actual'].value_counts().unstack(fill_value=0)\n",
                "bin_counts.plot(kind='bar', ax=axes[1, 1], color=['green', 'red'])\n",
                "axes[1, 1].set_title('Actual Outcomes by Probability Bin')\n",
                "axes[1, 1].set_xlabel('Probability Bin')\n",
                "axes[1, 1].set_ylabel('Count')\n",
                "axes[1, 1].legend(['No Churn', 'Churn'])\n",
                "axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Store Predictions for Future Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize prediction store\n",
                "store = PredictionStore()\n",
                "\n",
                "# Store predictions with actual outcomes\n",
                "stored_count = 0\n",
                "for idx in range(len(test_data)):\n",
                "    customer_id = f\"TEST_{idx:04d}\"\n",
                "    prediction = float(y_pred_proba[idx])\n",
                "    actual = int(y_test.iloc[idx])\n",
                "    \n",
                "    store.store(\n",
                "        customer_id=customer_id,\n",
                "        prediction=prediction,\n",
                "        actual_outcome=actual,\n",
                "        model_version=predictor.model_version\n",
                "    )\n",
                "    stored_count += 1\n",
                "\n",
                "print(f\"✓ Stored {stored_count} predictions\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Retrieve and analyze stored predictions\n",
                "stored_predictions = store.retrieve(model_version=predictor.model_version)\n",
                "\n",
                "print(f\"Retrieved {len(stored_predictions)} predictions\")\n",
                "print(f\"\\nSample stored predictions:\")\n",
                "print(stored_predictions.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Performance Monitoring and Alerts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize alert service\n",
                "alert_service = AlertService()\n",
                "\n",
                "# Check for performance degradation\n",
                "alert = alert_service.check_performance(metrics['recall'], threshold=0.85)\n",
                "\n",
                "if alert:\n",
                "    print(f\"⚠ ALERT: {alert}\")\n",
                "else:\n",
                "    print(\"✓ No performance alerts - model is performing within acceptable range\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate monitoring over time\n",
                "# In production, you would track these metrics over days/weeks\n",
                "\n",
                "monitoring_data = {\n",
                "    'date': pd.date_range(start='2025-01-01', periods=10, freq='D'),\n",
                "    'recall': [0.87, 0.86, 0.85, 0.84, 0.83, 0.82, 0.81, 0.80, 0.79, 0.78],\n",
                "    'precision': [0.52, 0.51, 0.50, 0.49, 0.48, 0.47, 0.46, 0.45, 0.44, 0.43],\n",
                "    'f1_score': [0.65, 0.64, 0.63, 0.62, 0.61, 0.60, 0.59, 0.58, 0.57, 0.56]\n",
                "}\n",
                "\n",
                "monitoring_df = pd.DataFrame(monitoring_data)\n",
                "\n",
                "# Plot performance over time\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "axes[0].plot(monitoring_df['date'], monitoring_df['recall'], marker='o', linewidth=2)\n",
                "axes[0].axhline(y=0.85, color='red', linestyle='--', label='Threshold')\n",
                "axes[0].set_xlabel('Date')\n",
                "axes[0].set_ylabel('Recall')\n",
                "axes[0].set_title('Recall Over Time')\n",
                "axes[0].legend()\n",
                "axes[0].grid(alpha=0.3)\n",
                "axes[0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "axes[1].plot(monitoring_df['date'], monitoring_df['precision'], marker='o', linewidth=2, color='orange')\n",
                "axes[1].set_xlabel('Date')\n",
                "axes[1].set_ylabel('Precision')\n",
                "axes[1].set_title('Precision Over Time')\n",
                "axes[1].grid(alpha=0.3)\n",
                "axes[1].tick_params(axis='x', rotation=45)\n",
                "\n",
                "axes[2].plot(monitoring_df['date'], monitoring_df['f1_score'], marker='o', linewidth=2, color='green')\n",
                "axes[2].set_xlabel('Date')\n",
                "axes[2].set_ylabel('F1-Score')\n",
                "axes[2].set_title('F1-Score Over Time')\n",
                "axes[2].grid(alpha=0.3)\n",
                "axes[2].tick_params(axis='x', rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Performance Trend Analysis:\")\n",
                "print(f\"  Recall degradation: {monitoring_df['recall'].iloc[0] - monitoring_df['recall'].iloc[-1]:.3f}\")\n",
                "print(f\"  Alert triggered on: {monitoring_df[monitoring_df['recall'] < 0.85]['date'].min()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Model Version Management"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize repository\n",
                "repo = ModelRepository()\n",
                "\n",
                "# List all versions\n",
                "versions = repo.list_versions()\n",
                "\n",
                "print(f\"Total model versions: {len(versions)}\\n\")\n",
                "\n",
                "# Create comparison DataFrame\n",
                "version_data = []\n",
                "for v in versions:\n",
                "    version_data.append({\n",
                "        'version': v.version,\n",
                "        'recall': v.metadata.get('recall', None),\n",
                "        'precision': v.metadata.get('precision', None),\n",
                "        'f1_score': v.metadata.get('f1_score', None),\n",
                "        'timestamp': v.metadata.get('timestamp', None)\n",
                "    })\n",
                "\n",
                "version_df = pd.DataFrame(version_data)\n",
                "print(\"Model Version Comparison:\")\n",
                "print(version_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize version performance\n",
                "if len(version_df) > 1:\n",
                "    fig, ax = plt.subplots(figsize=(12, 6))\n",
                "    \n",
                "    x = range(len(version_df))\n",
                "    width = 0.25\n",
                "    \n",
                "    ax.bar([i - width for i in x], version_df['recall'], width, label='Recall', alpha=0.8)\n",
                "    ax.bar(x, version_df['precision'], width, label='Precision', alpha=0.8)\n",
                "    ax.bar([i + width for i in x], version_df['f1_score'], width, label='F1-Score', alpha=0.8)\n",
                "    \n",
                "    ax.axhline(y=0.85, color='red', linestyle='--', label='Recall Threshold')\n",
                "    ax.set_xlabel('Model Version')\n",
                "    ax.set_ylabel('Score')\n",
                "    ax.set_title('Model Performance Across Versions')\n",
                "    ax.set_xticks(x)\n",
                "    ax.set_xticklabels(version_df['version'], rotation=45, ha='right')\n",
                "    ax.legend()\n",
                "    ax.grid(alpha=0.3, axis='y')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Model Rollback Simulation\n",
                "\n",
                "Demonstrate how to rollback to a previous version if needed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get current version\n",
                "current_version = repo.get_latest_version()\n",
                "print(f\"Current deployed version: {current_version}\")\n",
                "\n",
                "# Simulate rollback (uncomment to actually perform rollback)\n",
                "# if len(versions) > 1:\n",
                "#     previous_version = versions[-2].version\n",
                "#     print(f\"\\nRolling back to: {previous_version}\")\n",
                "#     repo.rollback(previous_version)\n",
                "#     print(\"✓ Rollback complete\")\n",
                "#     \n",
                "#     # Verify\n",
                "#     new_current = repo.get_latest_version()\n",
                "#     print(f\"New deployed version: {new_current}\")\n",
                "\n",
                "print(\"\\nNote: Rollback code is commented out to prevent accidental execution.\")\n",
                "print(\"Uncomment the code above to perform an actual rollback.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 11: Detailed Classification Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate detailed classification report\n",
                "print(\"Detailed Classification Report:\")\n",
                "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
                "\n",
                "# Additional metrics\n",
                "from sklearn.metrics import matthews_corrcoef, cohen_kappa_score\n",
                "\n",
                "mcc = matthews_corrcoef(y_test, y_pred)\n",
                "kappa = cohen_kappa_score(y_test, y_pred)\n",
                "\n",
                "print(f\"\\nAdditional Metrics:\")\n",
                "print(f\"  Matthews Correlation Coefficient: {mcc:.4f}\")\n",
                "print(f\"  Cohen's Kappa: {kappa:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "In this notebook, we:\n",
                "\n",
                "1. ✓ Loaded and evaluated a trained model\n",
                "2. ✓ Computed comprehensive performance metrics\n",
                "3. ✓ Analyzed confusion matrices\n",
                "4. ✓ Examined ROC curves and AUC\n",
                "5. ✓ Checked model calibration\n",
                "6. ✓ Analyzed probability distributions\n",
                "7. ✓ Stored predictions for validation\n",
                "8. ✓ Set up performance monitoring and alerts\n",
                "9. ✓ Compared model versions\n",
                "10. ✓ Demonstrated rollback capabilities\n",
                "\n",
                "## Key Takeaways\n",
                "\n",
                "- **Continuous Monitoring**: Track metrics over time to detect degradation early\n",
                "- **Multiple Metrics**: Don't rely on a single metric - use precision, recall, F1, AUC together\n",
                "- **Calibration Matters**: Well-calibrated probabilities are crucial for decision-making\n",
                "- **Version Control**: Maintain multiple model versions for quick rollback\n",
                "- **Alert Systems**: Automated alerts help catch issues before they impact business"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Best Practices for Production Monitoring\n",
                "\n",
                "1. **Set Up Automated Monitoring**\n",
                "   - Schedule daily/weekly evaluation jobs\n",
                "   - Track metrics in a time-series database\n",
                "   - Set up email/Slack alerts for degradation\n",
                "\n",
                "2. **Monitor Data Drift**\n",
                "   - Track feature distributions over time\n",
                "   - Compare production data to training data\n",
                "   - Alert on significant distribution shifts\n",
                "\n",
                "3. **A/B Testing**\n",
                "   - Test new models on a subset of traffic\n",
                "   - Compare performance before full deployment\n",
                "   - Gradually roll out improvements\n",
                "\n",
                "4. **Feedback Loop**\n",
                "   - Collect actual outcomes for predictions\n",
                "   - Retrain models with new data\n",
                "   - Continuously improve performance\n",
                "\n",
                "5. **Documentation**\n",
                "   - Document model changes and reasons\n",
                "   - Track business impact of model updates\n",
                "   - Maintain audit trail for compliance"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}